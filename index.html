<!DOCTYPE html>
<html>
    <head>
        <title>Artificial Neural Networks</title>
        <link rel="stylesheet" href="css/style.css" type="text/css">
        <!-- <link rel="stylesheet" href="css/bootstrap.css" type="text/css"> -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script src="js/script.js"></script>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BmbxuPwQa2lc/FVzBcNJ7UAyJxM6wuqIj61tLrc4wSX0szH/Ev+nYRRuWlolflfl" crossorigin="anonymous">
        <!-- <script src ="js/bootstrap.js"></script>
        <script src ="js/bootstrap.min.js"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    </head>
    <body>
        <div class = "parallax" style = "background-image: url(brain.jpeg); color:white">
            <div class="content1">
                <h1>Artificial Neural Networks</h1>
                <h2>By: Lucas Khan | 12-G</h2>
            </div>
        </div>
        <div class="cont", style="background-image: url(background.jpeg);">
            <div class="parallax">
                <div class="maintext container main">
                    <br/>
                    <br/>
                    <h2 id="test">What are neural networks?</h2>
                    <p>An <b>artificial neural network</b> (ANN) is a computational model inspired by the physiology of animal brains.
                    Similar to the way neurons in the brain send signals to each other, an ANN consists of interconnected nodes
                    (neurons) that map a certain input to a certain output or prediction.</p>
        
                    <br/>
                    <p>A commonly used form of neural network is the multilayer perceptron (MLP), which contains multiple layers of neurons. As a form of artificial intelligence, MLPs are applied to a multitude of problems and fields, such as facial recognition,
                        automatic translators, and self-driving cars. For problems like these, using conventional algorithms or computer programs
                        is practically impossible. For example, it would take millions of lines of code and a sizable amount of effort to describe
                        to a computer what a face looks like or how to adequately translate every word in a language. AI algorithms, meanwhile,
                        get around this by learning from vast amounts of data in a process called <b>training</b>.
                    </p>
                    <p>To illustrate this, we will use a common sample problem: predicting the species of a penguin (either Gentoo or Adelie) based on their bill length,
                        flipper length, and body mass. For simplicity, this is be a binary classification problem, although
                        more classes may be added if there is data for it.
                    </p>
                    <p class="down">scroll down &or;</p>

                    <!-- <p>Some common applications of MLPs include:</p> -->


                </div>
            </div>
            <div class="cent" style="width:100vw;display:block;margin-left:auto;margin-right:auto;">
                <div id="carousel" class="carousel slide" data-bs-ride="carousel" data-bs-keyboard="true" data-bs-interval="false">
                    <div class="carousel-inner">
                      <div class="carousel-item active">
                        <img class="d-block w-75" src="slide1.png" alt="First slide" style="margin-left:auto;margin-right:auto;">
                      </div>
                      <div class="carousel-item">
                        <img class="d-block w-75" src="slide2.png" alt="Second slide" style="margin-left:auto;margin-right:auto;">
                      </div>
                      <div class="carousel-item">
                        <img class="d-block w-75" src="slide3.png" alt="Third slide" style="margin-left:auto;margin-right:auto;">
                      </div>
                    </div>
                    <button class="carousel-control-prev" type="button" data-bs-target="#carousel" data-bs-slide="prev">
                        <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                        <span class="visually-hidden">Previous</span>
                      </button>
                      <button class="carousel-control-next" type="button" data-bs-target="#carousel" data-bs-slide="next">
                        <span class="carousel-control-next-icon" aria-hidden="true"></span>
                        <span class="visually-hidden">Next</span>
                      </button>
                  </div>
            </div>
            <div class = "parallax">
                <div class="maintext container main">
                    <br/><br/><br/>
                    <h2>Backpropagation: how neural networks learn</h2>
                    <p>
                        We previously established that neural networks "learn" by having their weights and biases optimized.
                        In multilayer perceptrons, this is done using a process called backpropagation, which uses partial derivatives to minimize error.
                    </p>
                    <!-- <div class="text-center">
                        <figure class="figure">
                            <img src="xkcd.png" class="figure-img img-fluid rounded" alt="...">
                            <figcaption class="figure-caption"><a href="https://xkcd.com/1838/">image source</a></figcaption>
                        </figure>
                    </div> -->
                    <h3>The cost function</h3>
                    <p>MLPs commonly use <b>least mean squares</b> to measure the error of a particular configuration of weights.
                    The degree of error of output neuron \( j \) in predicting the \(n\)th datapoint in a training dataset is given by
                    </p>

                    <p class = "equation">
                        \( \displaystyle \mathcal{E}(n)=\frac{1}{2}\sum_j \left[\hat{y}_j (n) - y_j (n) \right]^2 \)
                    </p>

                    <p>Where \( \hat{y}_j \) is the expected value for neuron \(j\) in an MLP's output layer while \(y_j\) is its actual value. (Note
                        that \(n\) is not being multiplied to either of the two; it just refers to the \(n\)th datapoint.)
                    </p>

                    <div class="row w-100">
                        <div class="col-3">
                            <img src="image4.png" style="width:100%">
                        </div>
                        <div class="col-9">
                            <br/><br/>
                            <p>For example, if the MLP depicted to the left is correct in predicting "Gentoo" for the \(n\)th datapoint in its training dataset, the expected values \( \hat{y}_j \)
                                are 1 for the "Gentoo" neuron and 0 for the "Adelie" neuron. As such, its error is given by:
                            </p>
                            <p class = "equation">
                                \( \displaystyle \mathcal{E}(n)=\frac{1}{2} \left[ (1 - 0.82)^2 + (0 - 0.42)^2 \right] \)
                            </p>
                        </div>
                    </div>
                    
                    <h3>Gradient descent</h3>
                    <p>To optimize an MLP's weights, we must measure how it changes with regard to its error. Here, the change \( \Delta w_{ji} (n) \) of the weight
                        connecting neurons \( i \) and \( j \), where \( j \) is in the succeeding layer, is given by
                    </p>
                    <p class="equation">
                        \( \displaystyle \Delta w_{ji} (n) = -\eta\frac{\partial\mathcal{E}(n)}{\partial v_j(n)} y_i(n) \)
                    </p>
                    <p>where \( v_j(n) \) is the sum of all the weights connected to neuron \( j \), \( y_i \) is the output of the previous neuron \( i \), and
                        \( \eta \) is the learning rate, a constant selected by the user.
                    </p>

                    Since gradient descent is an iterative process, the weights are updated for every \(n\)th datapoint in the training dataset. This means that
                    as training goes on, the MLP's error gradually converges to a local minimum, increasing its accuracy.
                    
                    <div class="text-center">
                        <figure class="figure">
                            <img class="figure-img img-fluid rounded" src="gradient descent.png" style="width:50vw">
                            <figcaption class="figure-caption"><a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/">image source</a></figcaption>
                        </figure>
                    </div>
                    <br/><br/>
                </div>
            </div>
        </div>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/js/bootstrap.bundle.min.js" integrity="sha384-b5kHyXgcpbZJO/tY9Ul7kGkf1S0CWuKcCD38l8YkeH8z8QjE0GmW1gYU5S9FOnJ0" crossorigin="anonymous"></script>
    </body>
</html>
